# Business Case: Гибридная AI-система для категоризации транзакций и финансовых рекомендаций

## 1. Проблема (Problem Statement)

В современных банковских приложениях клиентам предоставляется аналитика расходов и персональные советы. Однако существующие решения сталкиваются с рядом проблем:

- **Категоризация транзакций** часто выполняется по неточным правилам (MCC-коды, ключевые слова) или с помощью простых ML-моделей, которые не учитывают контекст и могут ошибаться в нестандартных случаях.
- **Персонализированные рекомендации** либо отсутствуют, либо шаблонны, не учитывают реальное поведение клиента и не используют всю доступную информацию.
- **Операционные затраты** на ручную модерацию сомнительных транзакций остаются высокими, а полностью автоматизированное решение с использованием LLM для всех транзакций слишком дорого и медленно.

Таким образом, требуется система, которая:
- Обеспечивает высокую точность категоризации (≥95%) при минимальных затратах.
- Даёт персонализированные, контекстно-зависимые финансовые советы.
- Масштабируется и остаётся экономически эффективной.

## 2. Предлагаемое решение (Proposed Solution)

Разработать **гибридную событийно-ориентированную архитектуру**, объединяющую:

1. **Быстрый ML-классификатор** на Java (библиотека Smile) для массовой обработки транзакций с высокой уверенностью (пока будет временное решение с if-else).
2. **AI-агентов на базе LLM** для обработки сложных случаев и генерации рекомендаций.
3. **Асинхронную шину событий (Apache Kafka)** для надёжной и масштабируемой передачи данных.
4. **Postgres с JSONB** для хранения ответов AI агента.
5. **Кэширование в SQL** для ускорения повторяющихся запросов.

### 2.1. Ключевые компоненты

- **Модель транзакции:** содержит поля `id`, `user_id`, `amount`, `timestamp`, `description`, `merchant_name`, `mcc_code`, `account_id` (валюта фиксирована, модель расширяема).
- **ML-модуль (Java + Smile):** градиентный бустинг или случайный лес, обученный на исторических данных. Обрабатывает ~90% транзакций, выдаёт категорию и уверенность.
- **Transaction Classifier Agent (LLM):** получает транзакции, в которых ML-модель не уверена (confidence < порога). Анализирует описание, историю пользователя, возвращает категорию и пояснение.
- **Financial Coach Agent (LLM + tools):** запускается по расписанию или по запросу, анализирует последние 30 дней трат, использует инструменты (функции) для получения агрегированных данных, генерирует персонализированные советы.
- **Ручная проверка:** транзакции, не классифицированные ни ML, ни LLM, направляются оператору (в перспективе).

### 2.2. Бизнес-процесс (схема)

1. Клиент совершает транзакцию → данные попадают в **топик Kafka** `raw-transactions`.
2. **Core service** читает топик, вызывает **ML-модуль**:
   - Если уверенность > 0.9 → категория сохраняется в БД.
   - Иначе → транзакция отправляется в топик `llm-reasoning-requests`.
3. **Transaction Reasoning Agent** обрабатывает запрос:
   - Использует LLM с контекстом (история пользователя, merchant_name и т.д.).
   - Возвращает категорию + confidence + пояснение.
   - Результат сохраняется в БД, публикуется событие.
4. **Financial Coach Agent** (запускается по расписанию либо вручную):
   - Читает агрегированные данные за последние 30 дней из кэша/БД.
   - Вызывает инструменты для получения конкретных сумм по категориям, сравнения с прошлым периодом, обнаружения всплесков.
   - Формирует совет и публикует в топик `recommendations`.
5. **Сервис уведомлений** отправляет советы клиенту в telegram бот.

## 3. Ключевые метрики успеха (Success Metrics)
В рамках MVP confidence LLM используется как эвристическая оценка. 
| Метрика | As-Is (базовое правило/ML) | To-Be (гибрид) | Способ измерения |
|--------|----------------------|----------------|-------------------|
| Точность категоризации | ~85% (только ML) | **>95%** (ML + LLM) | Сравнение с эталонной разметкой на тестовом наборе |
| Доля транзакций, обработанных ML | 100% | **~90%** | Счётчики в Kafka/логах |
| Доля транзакций, обработанных LLM | 0% | **~9.5%** | Счётчики в Kafka/логах |
| Доля транзакций, требующих ручной проверки | >5% | **<0.5%** | Логи ручной модерации |
| Среднее время обработки транзакции (p95) | <50 мс (синхронно) | **<2 с** (асинхронно, включая LLM) | Метрики в Prometheus / логи |
| Пропускная способность (RPS) | до 500 | **>1500** | Нагрузочное тестирование |
| Точность/полезность рекомендаций (оценка пользователей) | N/A (нет) | **>80%** (положительные отзывы) | A/B тест, анкетирование |
| Стоимость обработки 1 млн транзакций | Базовая (ML-инференс) | **+15%** к базовой (за счёт LLM для 10% транзакций) | Расчёт на основе тарифов API LLM |

*P.S. Калибровка вероятностей и анализ надёжности будут предметом дальнейших исследований.*
## 4. Обоснование выбора архитектуры (Architecture Drivers)

### Почему гибрид ML + LLM?
- **ML** обеспечивает дешёвую и быструю обработку основной массы транзакций.
- **LLM** подключается только для сложных случаев (низкая уверенность ML), где требуется понимание контекста, неявных паттернов или обработка новых merchant'ов. Это баланс между стоимостью и качеством.
- Такой подход соответствует современным практикам (например, кейс Algoan, где LLM используется как "ассистент учителя" для разметки сложных случаев).

### Почему Kafka?
- Асинхронность необходима, так как LLM работает с задержкой 1-3 секунды. Блокировать пользователя на это время нельзя.
- Kafka гарантирует надёжную доставку, позволяет масштабировать обработчики и даёт возможность повторной обработки при сбоях.

*P.S. Kafka используется для демонстрации событийно-ориентированной архитектуры и масштабируемости, 
даже если нагрузка MVP не требует полноценного брокера.*

### Почему Java (Smile) для ML?
- Smile — зрелая библиотека с широким набором алгоритмов, написанная на Java, что позволяет интегрировать ML непосредственно в Spring-приложение без дополнительных микросервисов и межязыкового взаимодействия.
- Это упрощает инфраструктуру и укладывается в требование использовать Java-модуль.

*P.S. начну делать после реализации целевых модулей сервиса, пока временное решение будет из обычного if else case*

### Почему Postgres с JSONB?
- Для структурированных данных (транзакции, пользователи) — реляционная модель.
- Для хранения разнородных данных от LLM (промпты, ответы, метаданные, уверенность) — JSONB даёт гибкость без необходимости менять схему. Это позволяет в будущем анализировать и улучшать работу AI.

### Почему два агента?
- Разделение ответственности: Transaction Classifier Agent занимается классификацией, Financial Coach Agent — рекомендациями. Это упрощает разработку, тестирование и замену каждого агента независимо.
- Coach Agent использует **инструменты (functions)**, что приближает его к концепции ReAct-агентов и позволяет давать точные, основанные на данных советы, а не просто генерировать текст.

## 5. Ожидаемый эффект (Expected Outcomes)

- **Для клиента:** точная аналитика расходов и действительно полезные персонализированные советы → повышение лояльности и вовлечённости.
- **Для банка:** снижение операционных затрат на ручную модерацию, увеличение cross-sell за счёт релевантных предложений, возможность монетизации рекомендаций (партнёрские программы).
- **Для системы:** масштабируемость, отказоустойчивость, гибкость добавления новых AI-функций.

## 6. Риски и их митигация

| Риск | Митигация |
|------|-----------|
| Низкое качество LLM-классификации (галлюцинации, неверные категории) | Использовать проверенные модели (GPT-4o, YandexGPT, Claude), добавить валидацию через confidence threshold, логировать все ответы для анализа |
| Высокая стоимость вызовов LLM | Кэшировать результаты для часто повторяющихся merchant'ов, использовать локальные модели (Llama.cpp, Mistral) для удешевления |
| Сложность отладки асинхронных процессов | Внедрить сквозную трассировку (correlation-id), детальное логирование, использовать тестовые контейнеры для интеграционных тестов |
| Риск переобучения ML-модели | Регулярное переобучение на новых данных, мониторинг дрейфа данных |
| Недостаточная точность ML на редких категориях | Обучать модель с учётом дисбаланса классов, использовать взвешенные метрики |

## 7. Заключение

Предлагаемая гибридная архитектура сочетает преимущества классического машинного обучения (скорость, низкая стоимость) и больших языковых моделей (глубокое понимание контекста, гибкость). Это позволяет достичь высокой точности категоризации и качества рекомендаций при сохранении приемлемых затрат и масштабируемости. Проект полностью соответствует современным трендам в финтехе и может служить основой для магистерской диссертации.